{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBPP3jic3u_C"
      },
      "source": [
        "# Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx7S069c3T0e"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate accelerate\n",
        "!pip install causal-conv1d>=1.1.0\n",
        "!pip install mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPyEBwB03x9x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KY5z62dS4P1r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer\n",
        "from transformers import AutoTokenizer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcYsCoMV5dcf"
      },
      "source": [
        "# Dowload dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TY2tnwt5cng"
      },
      "outputs": [],
      "source": [
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKyHvtdt5m-S"
      },
      "source": [
        "# Build custom Mamba Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wLg0IAt65pTh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        " # Config class of Mamba\n",
        "class MambaConfig:\n",
        "    d_model: int = 2560\n",
        "\n",
        "    n_layer: int = 64\n",
        "    vocab_size: int = 50277\n",
        "    ssm_cfg: dict = field(default_factory=dict)\n",
        "    rms_norm: bool = True\n",
        "    residual_in_fp32: bool = True\n",
        "    fused_add_norm: bool = True\n",
        "    pad_vocab_size_multiple: int = 8\n",
        "\n",
        "    def to_json_string(self):\n",
        "        return json.dumps(asdict(self))\n",
        "    def to_dict(self):\n",
        "        return asdict(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LjrBVrZl55F_"
      },
      "outputs": [],
      "source": [
        "# class head\n",
        "class MambaClassificationHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes, **kwargs):\n",
        "\n",
        "        super(MambaClassificationHead, self).__init__()\n",
        "        self.classification_head = nn.Linear(d_model, num_classes,**kwargs)\n",
        "    def forward(self, hidden_states):\n",
        "        return self.classification_head(hidden_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ro-epX56yv2g"
      },
      "outputs": [],
      "source": [
        "class MambaTextClassification(MambaLMHeadModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: MambaConfig,\n",
        "        initializer_cfg = None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    )-> None:\n",
        "        super().__init__(config, initializer_cfg, device, dtype)\n",
        "\n",
        "        self.classification_head = MambaClassificationHead(d_model=config.d_model, num_classes=2)\n",
        "\n",
        "        del self.lm_head\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Truyền input_ids qua model gốc để nhận hidden_states.\n",
        "        hidden_states = self.backbone(input_ids)\n",
        "        # Lấy trung bình của hidden_states theo chiều thứ 2 để tạo ra [CLS] feature đại điện\n",
        "        mean_hidden_states = hidden_states.mean(dim=1)\n",
        "        # Đưa mean_hidden_states qua đầu phân loại để nhận logits.\n",
        "        logits = self.classification_head(mean_hidden_states)\n",
        "        if labels is None:\n",
        "            ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"logits\"])\n",
        "            return ClassificationOutput(logits=logits)\n",
        "        else:\n",
        "            ClassificationOutput = namedtuple(\"ClassificationOutput\", [\"loss\", \"logits\"])\n",
        "            # Sử dụng hàm mất mát CrossEntropyLoss để tính loss.\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "            return ClassificationOutput(loss=loss, logits=logits)\n",
        "    def predict(self, text, tokenizer, id2label=None):\n",
        "        input_ids = torch.tensor(tokenizer(text)['input_ids'], device='cuda')[None]\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(input_ids).logits[0]\n",
        "            label = np.argmax(logits.cpu().numpy())\n",
        "        if id2label is not None:\n",
        "            return id2label[label]\n",
        "        else:\n",
        "            return label\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
        "        # Tải cấu hình từ model đã được train trước đó.\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        config = MambaConfig(**config_data)\n",
        "        # Khởi tạo model từ cấu hình và chuyển nó đến thiết bị và kiểu dữ liệu mong muốn.\n",
        "        model = cls(config, device=device, dtype=dtype, **kwargs)\n",
        "        # Tải trạng thái model đã được train trước đó.\n",
        "        model_state_dict = load_state_dict_hf(pretrained_model_name,\n",
        "        device=device, dtype=dtype)\n",
        "        model.load_state_dict(model_state_dict, strict=False)\n",
        "        # In ra các tham số embedding mới được khởi tạo.\n",
        "        print(\"Newly initialized embedding:\", set(model.state_dict().keys())- set(model_state_dict.keys()))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g5jehpJ0DpG"
      },
      "outputs": [],
      "source": [
        "# Tải model Mamba từ model đã được train trước đó.\n",
        "model = MambaTextClassification.from_pretrained(\"state-spaces/mamba-130m\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Tải tokenizer của model Mamba từ model gpt-neox-20b.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "# Đặt id của token pad bằng id của token eos trong tokenizer.\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUvDY0P70Jj_"
      },
      "outputs": [],
      "source": [
        "# Tạo chức năng tiền xử lý để mã hóa văn bản và cắt bớt các chuỗi không dài hơn độ dài đầu vào tối đa của mã thông báo\n",
        "def preprocess_function(examples):\n",
        "  samples = tokenizer(examples[\"text\"], truncation=True)\n",
        "  # Không cần attention_mask\n",
        "  # Cụ thể hơn về token masking của mamba có thể tham khảo: https://\n",
        "  github.com/state-spaces/mamba/issues/49\n",
        "  samples.pop('attention_mask')\n",
        "  return samples\n",
        "# Thực hiện mã hóa văn bản\n",
        "tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "\n",
        "# Set seed cho hàm random\n",
        "random.seed(42)\n",
        "\n",
        "# Tạo tập train và test\n",
        "train_dataset = tokenized_imdb[\"train\"]\n",
        "test_dataset = tokenized_imdb[\"test\"]\n",
        "\n",
        "# Tạo tập evaluation để đánh giá trong lúc train\n",
        "# Do số lượng tập test lớn nên chỉ lấy mẫu 1% tập dữ liệu test để đánh giá\n",
        "total_samples = len(test_dataset)\n",
        "eval_samples = int(0.1 * total_samples)\n",
        "eval_indices = random.sample(range(total_samples), eval_samples)\n",
        "eval_dataset = test_dataset.select(eval_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8v2WxSr0NBu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Tải module \"accuracy\" từ thư viện evaluate.\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "# Định nghĩa hàm compute_metrics để tính các độ đo hiệu suất (metrics) cho việc đánh giá model.\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  # Lấy chỉ số của lớp có xác suất cao nhất trong predictions.\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  # Sử dụng module \"accuracy\" để tính độ chính xác dựa trên predictions và labels.\n",
        "  return accuracy.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Nqbv_h0Qnp"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa tên project để log thông tin quá trình train trên wandb\n",
        "# os.environ[\"WANDB_PROJECT\"] = \"mamba_tutorial\"\n",
        "\n",
        "# Định nghĩa các tham số train trong class TrainingArguments.\n",
        "# Cụ thể hơn về các tham số hỗ trợ có thể tham khảo: https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"mamba_text_classification\", # Tên folder output\n",
        "  learning_rate=5e-5,\n",
        "  per_device_train_batch_size=4, # Số lượng train sample trên mỗi device\n",
        "  per_device_eval_batch_size=16, # Số lượng eval sample trên mỗi device\n",
        "  num_train_epochs=1, # Số epoch train\n",
        "  report_to=\"none\", # \"wandb\" nếu muốn log kết quả\n",
        "  warmup_ratio=0.01, # Tỉ lệ tăng dần lr trong giai đoạn warmup\n",
        "  lr_scheduler_type=\"cosine\", # Loại scheduler để giảm lr\n",
        "  evaluation_strategy=\"steps\", # Xác định metric đánh giá sau mỗi số bước\n",
        "  eval_steps=0.1, # Số bước giữa các đợt đánh giá\n",
        "  save_strategy=\"steps\", # Xác định khi nào lưu checkpoint\n",
        "  save_steps=0.1, # Số bước giữa các lần lưu checkpoint\n",
        "  logging_strategy=\"steps\", # Xác định khi nào in thông tin log\n",
        "  logging_steps=1, # Số bước giữa các lần in thông tin log\n",
        "  push_to_hub=True, # Đẩy kết quả lên Hub\n",
        "  load_best_model_at_end=True, # Load model có kết quả evaluation tốt nhất trong quá trình train\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxI61qHN0S1f"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa một class MambaTrainer kế thừa từ class Trainer.\n",
        "class MambaTrainer(Trainer):\n",
        "\n",
        "  # Định nghĩa hàm compute_loss để tính toán hàm mất mát trong quá trình train.\n",
        "  def compute_loss(self, model, inputs, return_outputs=False):\n",
        "      # Lấy giá trị input_ids và labels từ inputs.\n",
        "      input_ids = inputs.pop(\"input_ids\")\n",
        "      labels = inputs.pop('labels')\n",
        "      # Gọi hàm forward của model với input_ids và labels để nhận các kết quả.\n",
        "      outputs = model(input_ids=input_ids, labels=labels)\n",
        "      # Lấy giá trị loss từ kết quả của model.\n",
        "      loss = outputs.loss\n",
        "      # Trả về cả loss và outputs nếu return_outputs là True, ngược lại chỉ trả về loss.\n",
        "      return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "  # Định nghĩa hàm save_model để lưu model trong quá trình train.\n",
        "  def save_model(self, output_dir = None, _internal_call = False):\n",
        "      # Kiểm tra nếu thư mục lưu trữ không được chỉ định, sử dụng thư mục mặc định từ đối số ’args’.\n",
        "      if output_dir is None:\n",
        "          output_dir = self.args.output_dir\n",
        "      # Nếu thư mục đầu ra không tồn tại, tạo mới nó.\n",
        "      if not os.path.exists(output_dir):\n",
        "          os.makedirs(output_dir)\n",
        "      # Lưu trạng thái của model PyTorch vào file ’pytorch_model.bin’ trong thư mục đầu ra.\n",
        "      torch.save(self.model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
        "      # Lưu trạng thái của tokenizer vào thư mục đầu ra.\n",
        "      self.tokenizer.save_pretrained(output_dir)\n",
        "      # Lưu cấu hình của model vào file ’config.json’ trong thư mục đầu ra.\n",
        "      with open(f'{output_dir}/config.json', 'w') as f:\n",
        "          json.dump(self.model.config.to_dict(), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJWlDRF90T9P"
      },
      "outputs": [],
      "source": [
        "# Khởi tạo classs MambaTrainer để thực hiện quá trình train của\n",
        "\n",
        "trainer = MambaTrainer(\n",
        "  model=model, # Model cần train\n",
        "  train_dataset=train_dataset, # Dữ liệu train\n",
        "  eval_dataset=eval_dataset, # Dữ liệu đánh giá\n",
        "  tokenizer=tokenizer, # Tokenizer sử dụng để mã hóa dữ liệu\n",
        "  args=training_args, # Các tham số train đã được định nghĩa trước đó\n",
        "  compute_metrics=compute_metrics # Hàm tính các độ đo hiệu suất (metrics) cho đánh giá\n",
        " )\n",
        " # Bắt đầu quá trình train bằng cách gọi hàm train() trên classs\n",
        " trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOTNgIK80X7B"
      },
      "outputs": [],
      "source": [
        " # Đẩy model lên huggingface hub\n",
        "trainer.push_to_hub(commit_message=\"Training complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FhTad1o0Zzk"
      },
      "outputs": [],
      "source": [
        " # Thực hiện dự đoán trên tập dữ liệu validation\n",
        "outputs = trainer.predict(test_dataset)\n",
        "print(outputs.metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải model Mamba từ model đã được train trước đó.\n",
        "model = MambaTextClassification.from_pretrained(\"trinhxuankhai/\n",
        "mamba_text_classification\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Tải tokenizer của model Mamba từ model đã được train trước đó.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"trinhxuankhai/mamba_text_classification\")\n",
        "# Đặt id của token pad bằng id của token eos trong tokenizer.\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "Y7eSX7z6bkgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "text = imdb['test'][0]['text']\n",
        "label = imdb['test'][0]['label']\n",
        "response = model.predict(text, tokenizer, id2label)\n",
        "print(f'Classify: {text}\\nGT: {id2label[label]}\\nPredict: {response}')"
      ],
      "metadata": {
        "id": "GrBsQk4_bldB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwhqevV+xJ8nm36rMAY9G1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}